{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requiring module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")  # specify to ignore warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "sns.set(color_codes=True)\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "submission = pd.read_csv('./data/submission.csv')\n",
    "df_copy = test.copy()\n",
    "df_copy.date = pd.to_datetime(df_copy.date)\n",
    "df_copy.date = pd.to_datetime(df_copy.date.astype(str) + \" \" + df_copy.time, format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        store_id                date      time     card_id  amount  \\\n",
      "0              0 2016-08-01 00:28:15  00:28:15  bf33518373     125   \n",
      "1              0 2016-08-01 01:09:58  01:09:58  7a19a3a92f      90   \n",
      "2              0 2016-08-01 01:47:24  01:47:24  6f9fd7e241     150   \n",
      "3              0 2016-08-01 17:54:43  17:54:43  8bcf1d61b2     362   \n",
      "4              0 2016-08-01 18:48:53  18:48:53  6a722ce674     125   \n",
      "...          ...                 ...       ...         ...     ...   \n",
      "473387       199 2018-03-30 14:17:59  14:17:59  300d7bc922      65   \n",
      "473388       199 2018-03-30 19:01:54  19:01:54  3ab757718b      65   \n",
      "473389       199 2018-03-30 20:08:03  20:08:03  2d8e9e421c      65   \n",
      "473390       199 2018-03-30 20:11:58  20:11:58  22daeb334e     200   \n",
      "473391       199 2018-03-31 11:41:18  11:41:18  2e698f3302     500   \n",
      "\n",
      "        installments  days_of_week  holyday  \n",
      "0                NaN             0        0  \n",
      "1                NaN             0        0  \n",
      "2                NaN             0        0  \n",
      "3                NaN             0        0  \n",
      "4                NaN             0        0  \n",
      "...              ...           ...      ...  \n",
      "473387           NaN             4        0  \n",
      "473388           NaN             4        0  \n",
      "473389           NaN             4        0  \n",
      "473390           NaN             4        0  \n",
      "473391           NaN             5        0  \n",
      "\n",
      "[473392 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#열 순서에 주목. 음수값 삭제 함수에서 열 순서로 구분함.\n",
    "print(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log를 적용하기 위해서 음수값들을 과거시점의 양수값과 상계처리. \n",
    "#예측값은 총 매출이므로 실제 취소전 거래와 100% 매칭 시켜서 상계할 필요는 없다. 다만 이전 거래가 데이터 이전 시점인 경우 삭제한다.\n",
    "# Remove negative values from the data set.\n",
    "\n",
    "def reduce_noise_by_removing_neg_vals(df_copy):\n",
    "    df_pos = df_copy[df_copy.amount > 0]\n",
    "    df_neg = df_copy[df_copy.amount < 0]\n",
    "\n",
    "    start = datetime.now()\n",
    "    \n",
    "    #to_record -> df_neg 데이터프레임 내의 행을 반복하기 위한 iterator를 반환\n",
    "    for nega_i in df_neg.to_records()[:]:\n",
    "        store_i = nega_i[1]\n",
    "        date_i = nega_i[2]\n",
    "        card_i = nega_i[4]\n",
    "        amt_i = nega_i[5]\n",
    "        \n",
    "        #pos 데이터프레임에서 상계대상을 찾기위하여 필터링 \n",
    "        row_i = df_pos[df_pos.store_id == store_i]\n",
    "        row_i = row_i[row_i.card_id == card_i]\n",
    "        row_i = row_i[row_i.amount >= abs(amt_i)]\n",
    "        row_i = row_i[row_i.date <= date_i]\n",
    "        \n",
    "        #nega_i와 비교하여 amount가 정확히 같은 행이 있다면\n",
    "        if len(row_i[row_i.amount == abs(amt_i)]) > 0:\n",
    "            row_i = row_i[row_i.amount == abs(amt_i)] #amount가 정확히 같은 행만 골라내고\n",
    "            matched_row = row_i[row_i.date == max(row_i.date)] #amount가 정확히 같은 행 중 가장 최신의 행을 찾는다.\n",
    "            # df_pos.loc[matched_row.index, 'amount'] = 0\n",
    "            df_pos = df_pos.loc[~df_pos.index.isin(matched_row.index), :]#해당 행을 삭제한다.(상계처리)\n",
    "        #nega_i와 비교하여 amount가 정확히 같은 행이 없고, 대신 amount의 절대값이 큰 행이 있다면   \n",
    "        elif len(row_i[row_i.amount > abs(amt_i)]) > 0:\n",
    "            matched_row = row_i[row_i.date == max(row_i.date)]#그중에 가장 최신의 행을 찾는다.\n",
    "            df_pos.loc[matched_row.index, 'amount'] = matched_row.amount + amt_i #상계처리\n",
    "   \n",
    "    end = datetime.now()\n",
    "    time_took = (end - start).seconds / 60\n",
    "    \n",
    "    #굉장히 오래걸린다.\n",
    "    print(round(time_took, 2))#소요시간 로깅.(둘째자리에서 반올림)\n",
    "    return df_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오래걸림 실행하지 말것.\n",
    "#df_pos = reduce_noise_by_removing_neg_vals(df_copy)\n",
    "#편의를 위해 결과를 저장해 둠.\n",
    "#df_pos.to_csv('df_pos.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        store_id                 date      time     card_id  amount  \\\n",
      "0              0  2016-08-01 00:28:15  00:28:15  bf33518373     125   \n",
      "1              0  2016-08-01 01:09:58  01:09:58  7a19a3a92f      90   \n",
      "2              0  2016-08-01 01:47:24  01:47:24  6f9fd7e241     150   \n",
      "3              0  2016-08-01 17:54:43  17:54:43  8bcf1d61b2     362   \n",
      "4              0  2016-08-01 18:48:53  18:48:53  6a722ce674     125   \n",
      "...          ...                  ...       ...         ...     ...   \n",
      "457414       199  2018-03-30 14:17:59  14:17:59  300d7bc922      65   \n",
      "457415       199  2018-03-30 19:01:54  19:01:54  3ab757718b      65   \n",
      "457416       199  2018-03-30 20:08:03  20:08:03  2d8e9e421c      65   \n",
      "457417       199  2018-03-30 20:11:58  20:11:58  22daeb334e     200   \n",
      "457418       199  2018-03-31 11:41:18  11:41:18  2e698f3302     500   \n",
      "\n",
      "        installments  days_of_week  holyday  \n",
      "0                NaN             0        0  \n",
      "1                NaN             0        0  \n",
      "2                NaN             0        0  \n",
      "3                NaN             0        0  \n",
      "4                NaN             0        0  \n",
      "...              ...           ...      ...  \n",
      "457414           NaN             4        0  \n",
      "457415           NaN             4        0  \n",
      "457416           NaN             4        0  \n",
      "457417           NaN             4        0  \n",
      "457418           NaN             5        0  \n",
      "\n",
      "[457419 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "df_pos = pd.read_csv('df_pos.csv')\n",
    "print(df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>installments</th>\n",
       "      <th>days_of_week</th>\n",
       "      <th>holyday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>457419.000000</td>\n",
       "      <td>457419.000000</td>\n",
       "      <td>1633.000000</td>\n",
       "      <td>457419.000000</td>\n",
       "      <td>457419.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>91.080720</td>\n",
       "      <td>158.253230</td>\n",
       "      <td>3.372321</td>\n",
       "      <td>3.016151</td>\n",
       "      <td>0.041546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.978209</td>\n",
       "      <td>372.389229</td>\n",
       "      <td>2.616598</td>\n",
       "      <td>1.961984</td>\n",
       "      <td>0.199550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>142.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store_id         amount  installments   days_of_week  \\\n",
       "count  457419.000000  457419.000000   1633.000000  457419.000000   \n",
       "mean       91.080720     158.253230      3.372321       3.016151   \n",
       "std        54.978209     372.389229      2.616598       1.961984   \n",
       "min         0.000000       1.000000      2.000000       0.000000   \n",
       "25%        40.000000      40.000000      2.000000       1.000000   \n",
       "50%        80.000000      87.000000      3.000000       3.000000   \n",
       "75%       142.000000     170.000000      3.000000       5.000000   \n",
       "max       199.000000  100000.000000     24.000000       6.000000   \n",
       "\n",
       "             holyday  \n",
       "count  457419.000000  \n",
       "mean        0.041546  \n",
       "std         0.199550  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2016-08-01 00:02:42', '2016-08-01 00:12:43',\n",
      "               '2016-08-01 00:20:28', '2016-08-01 00:22:01',\n",
      "               '2016-08-01 00:24:27', '2016-08-01 00:25:19',\n",
      "               '2016-08-01 00:28:15', '2016-08-01 00:40:17',\n",
      "               '2016-08-01 00:57:32', '2016-08-01 01:03:18',\n",
      "               ...\n",
      "               '2018-03-31 23:35:46', '2018-03-31 23:37:16',\n",
      "               '2018-03-31 23:37:58', '2018-03-31 23:40:03',\n",
      "               '2018-03-31 23:43:16', '2018-03-31 23:44:29',\n",
      "               '2018-03-31 23:44:55', '2018-03-31 23:57:08',\n",
      "               '2018-03-31 23:57:46', '2018-03-31 23:59:34'],\n",
      "              dtype='datetime64[ns]', name='date', length=457393, freq=None)\n",
      "                     store_id  amount  holyday\n",
      "date                                          \n",
      "2016-08-01 00:02:42        96      90        0\n",
      "2016-08-01 00:12:43       125     260        0\n",
      "2016-08-01 00:20:28        88      50        0\n",
      "2016-08-01 00:22:01        88      30        0\n",
      "2016-08-01 00:24:27        60     122        0\n",
      "...                       ...     ...      ...\n",
      "2018-03-31 23:44:29        38     780        0\n",
      "2018-03-31 23:44:55       102     140        0\n",
      "2018-03-31 23:57:08        38     375        0\n",
      "2018-03-31 23:57:46       113      65        0\n",
      "2018-03-31 23:59:34        10      51        0\n",
      "\n",
      "[457393 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 같은 시간에 일어난 카드매출 집계처리\n",
    "df = df_pos.copy()\n",
    "test_groupby_date_store = df.groupby(['date', 'store_id'])['amount', 'holyday'].sum()\n",
    "test_groupby_date_store = test_groupby_date_store.reset_index()\n",
    "test_groupby_date_store = test_groupby_date_store.set_index('date')\n",
    "#test_groupby_date_store.index = pd.to_datetime(test_groupby_date_store.index)\n",
    "print(test_groupby_date_store.index)\n",
    "print(test_groupby_date_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199]\n"
     ]
    }
   ],
   "source": [
    "store_list = test_groupby_date_store.store_id.unique()\n",
    "store_list.sort()\n",
    "print(store_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #arima 모델 파라미터범위 결정시 사용한것으로 보임. (최종 모델에서는 사용하지 않음)\n",
    "# def adf_test(y):\n",
    "#     # perform Augmented Dickey Fuller test\n",
    "#     print('Results of Augmented Dickey-Fuller test:')\n",
    "#     dftest = adfuller(y, autolag='AIC')\n",
    "#     dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "#     for key, value in dftest[4].items():\n",
    "#         dfoutput['Critical Value ({})'.format(key)] = value\n",
    "#     print(dfoutput)\n",
    "\n",
    "# def ts_diagnostics(y, lags=None, title='', filename=''):\n",
    "#     '''\n",
    "#     Calculate acf, pacf, qq plot and Augmented Dickey Fuller test for a given time series\n",
    "#     '''\n",
    "#     if not isinstance(y, pd.Series):\n",
    "#         y = pd.Series(y)\n",
    "\n",
    "#     # weekly moving averages (5 day window because of workdays)\n",
    "#     rolling_mean = pd.Series.rolling(y, window=2).mean()\n",
    "#     rolling_std = pd.Series.rolling(y, window=2).std()\n",
    "\n",
    "#     fig = plt.figure(figsize=(14, 12))\n",
    "#     layout = (3, 2)\n",
    "#     ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "#     acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "#     pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "#     qq_ax = plt.subplot2grid(layout, (2, 0))\n",
    "#     hist_ax = plt.subplot2grid(layout, (2, 1))\n",
    "\n",
    "#     # time series plot\n",
    "#     y.plot(ax=ts_ax)\n",
    "#     rolling_mean.plot(ax=ts_ax, color='crimson')\n",
    "#     rolling_std.plot(ax=ts_ax, color='darkslateblue')\n",
    "#     plt.legend(loc='best')\n",
    "#     ts_ax.set_title(title, fontsize=24)\n",
    "\n",
    "#     # acf and pacf\n",
    "#     plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n",
    "#     plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n",
    "\n",
    "#     # qq plot\n",
    "#     sm.qqplot(y, line='s', ax=qq_ax)\n",
    "#     qq_ax.set_title('QQ Plot')\n",
    "\n",
    "#     # hist plot\n",
    "#     y.plot(ax=hist_ax, kind='hist', bins=25)\n",
    "#     hist_ax.set_title('Histogram')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # perform Augmented Dickey Fuller test\n",
    "#     print('Results of Dickey-Fuller test:')\n",
    "#     dftest = adfuller(y, autolag='AIC')\n",
    "#     dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "#     for key, value in dftest[4].items():\n",
    "#         dfoutput['Critical Value (%s)' % key] = value\n",
    "#     print(dfoutput)\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA 모델의 모수를 찾기 위한 함수 정의\n",
    "def get_optimal_params(y):\n",
    "    # Define the p, d and q parameters to take any value between 0 and 1\n",
    "\n",
    "    param_dict = {}\n",
    "    for param in pdq:\n",
    "        try:\n",
    "            #SARIMAX는 무엇일까?\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            )\n",
    "            results = mod.fit()\n",
    "            model = ARIMA(y, order=param)\n",
    "            results_ARIMA = model.fit(disp=-1)\n",
    "            results_ARIMA.summary()\n",
    "            param_dict[results.aic] = param\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    min_aic = min(param_dict.keys())\n",
    "    optimal_params = param_dict[min_aic]\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_main(input_df, sampling_period_days, fcst_period):\n",
    "    #sampling_period_days로 나누어떨어질 수 있도록 나머지에 해당하는 과거의 행은 버린다.\n",
    "    input_df = input_df[len(input_df) % sampling_period_days:].resample(str(sampling_period_days) + 'D').sum()\n",
    "    prob_of_no_sales = len(input_df[(input_df.amount == 0) | (input_df.amount.isna())]) / len(input_df)\n",
    "    ts_log = np.log(input_df.amount)\n",
    "    ts_log = ts_log[~ts_log.isin([np.nan, np.inf, -np.inf])]\n",
    "    \n",
    "    if len(ts_log) < min_period:\n",
    "        return None\n",
    "    if sampling_period_days >= 28:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "    elif sampling_period_days >= 14:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 14) / 365\n",
    "    else:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "\n",
    "    expected_loss_pct_lending = 1.00\n",
    "    optimal_prob = expected_loss_pct_lending / (expected_loss_pct_lending + expected_return_pct_lending)\n",
    "    optimal_z_score = st.norm.ppf(optimal_prob)\n",
    "\n",
    "    optimal_params = get_optimal_params(ts_log)\n",
    "    pdqs[store_i] = optimal_params\n",
    "\n",
    "    model = ARIMA(ts_log, order=optimal_params)\n",
    "    results_ARIMA = model.fit(disp=-1)\n",
    "    fcst = results_ARIMA.forecast(fcst_period)\n",
    "\n",
    "    fcst_means = fcst[0]\n",
    "    fcst_stds = fcst[1]\n",
    "    fcst_i = fcst_means - (fcst_stds * optimal_z_score)\n",
    "    fcst_i = sum(map(lambda x: np.exp(x) if np.exp(x) > 0 else 0, fcst_i))\n",
    "    #보정치\n",
    "    prediction_i = fcst_i * (1 - prob_of_no_sales)\n",
    "    #예측값 리턴\n",
    "    return prediction_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9557475778999738\n",
      "1.7033379408274\n"
     ]
    }
   ],
   "source": [
    "#프로그램 실행 전 실행시 사용되는 전역변수들 정의 (실제로 사용하진 않음.)\n",
    "sampling_p = 28\n",
    "predic_len = math.floor(100 / sampling_p)\n",
    "expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "expected_loss_pct_lending = 1.00\n",
    "\n",
    "#optimal Z는 무엇일까?\n",
    "optimal_prob = expected_loss_pct_lending / (expected_loss_pct_lending + expected_return_pct_lending)\n",
    "optimal_z_score = st.norm.ppf(optimal_prob)\n",
    "\n",
    "print(optimal_prob)\n",
    "print(optimal_z_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#프로그램 실행 전 실행시 사용되는 전역변수들 정의\n",
    "mean_period = 2 * 3 #14*2*3 = 84 <- 실행지점 맨 아래쪽 코드 참고.\n",
    "\n",
    "#다운샘플링 결과로 나온 데이터셋의 행의 수가 min_period 미만이면 arima를 적용하지 않음.\n",
    "min_period = 6\n",
    "\n",
    "'''\n",
    "PACF (편자기상관함수 http://bit.ly/2OeRail)\n",
    "\n",
    "를 통해 AR(0), AR(1), AR(2)를 확인. 하지만 AIC를 통해 ARIMA모델에 사용될 pqr 값들을 찾을 때 0~2의 값들을 통해 구한 pqr보다 0~1 사이로 찾은 pqr이 더 정확한 예측을 했기 때문에 pqr의 범위를 0~1로 설정 (line 141-160)\n",
    "'''\n",
    "max_pdq = 2\n",
    "p = d = q = range(0, max_pdq) #2는 범위에 포함되지 않음.\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "pdqs = dict()\n",
    "\n",
    "\n",
    "#제출파일 생성 준비\n",
    "submission_copy = submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            store_id  amount  holyday\n",
      "date                                 \n",
      "2016-08-01         0    2106        0\n",
      "2016-08-02         0    1528        0\n",
      "2016-08-03         0     560        0\n",
      "2016-08-04         0    1683        0\n",
      "2016-08-05         0    1686        0\n",
      "...              ...     ...      ...\n",
      "2018-03-27         0     255        0\n",
      "2018-03-28         0     821        0\n",
      "2018-03-29         0     472        0\n",
      "2018-03-30         0     694        0\n",
      "2018-03-31         0    1045        0\n",
      "\n",
      "[608 rows x 3 columns]\n",
      "            store_id  amount  holyday\n",
      "date                                 \n",
      "2016-08-21         0   28807       16\n",
      "2016-09-18         0   23770        8\n",
      "2016-10-16         0   27294        0\n",
      "2016-11-13         0   28752        0\n",
      "2016-12-11         0   25977       16\n",
      "2017-01-08         0   22449       17\n",
      "2017-02-05         0   24138        7\n",
      "2017-03-05         0   26465        0\n",
      "2017-04-02         0   32384        0\n",
      "2017-04-30         0   29708       29\n",
      "2017-05-28         0   38981        7\n",
      "2017-06-25         0   36256        0\n",
      "2017-07-23         0   35454        7\n",
      "2017-08-20         0   26307        0\n",
      "2017-09-17         0   33529       44\n",
      "2017-10-15         0   24483        0\n",
      "2017-11-12         0   25473        0\n",
      "2017-12-10         0   32003       23\n",
      "2018-01-07         0   30906        0\n",
      "2018-02-04         0   29167       17\n",
      "2018-03-04         0   30199        0\n",
      "date\n",
      "2016-08-21    10.268374\n",
      "2016-09-18    10.076180\n",
      "2016-10-16    10.214422\n",
      "2016-11-13    10.266463\n",
      "2016-12-11    10.164967\n",
      "2017-01-08    10.019001\n",
      "2017-02-05    10.091543\n",
      "2017-03-05    10.183578\n",
      "2017-04-02    10.385420\n",
      "2017-04-30    10.299172\n",
      "2017-05-28    10.570830\n",
      "2017-06-25    10.498360\n",
      "2017-07-23    10.475991\n",
      "2017-08-20    10.177590\n",
      "2017-09-17    10.420166\n",
      "2017-10-15    10.105734\n",
      "2017-11-12    10.145374\n",
      "2017-12-10    10.373585\n",
      "2018-01-07    10.338706\n",
      "2018-02-04    10.280793\n",
      "2018-03-04    10.315564\n",
      "Freq: 28D, Name: amount, dtype: float64\n",
      "     store_id   total_sales\n",
      "0           0  68426.936910\n",
      "1           1  11840.490783\n",
      "2           2  16208.571184\n",
      "3           3  26870.446220\n",
      "4           4  11752.421019\n",
      "..        ...           ...\n",
      "195       195  31237.923574\n",
      "196       196  31340.097798\n",
      "197       197     57.921562\n",
      "198       198   1061.787577\n",
      "199       199  18906.142306\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#실질적인 프로그램 실행 지점\n",
    "for store_i in store_list[:]:\n",
    "    #예측 결과 담길 변수 정의\n",
    "    prediction_i = None\n",
    "    \n",
    "    #해당 store_id에 해당하는 행들만 가져옴\n",
    "    test_df = test_groupby_date_store[test_groupby_date_store.store_id == store_i]\n",
    "    \n",
    "    #일자별로 행 집계(리샘플링)\n",
    "    test_df_daily = test_df.resample('D').sum()\n",
    "    \n",
    "    #다운샘플링 크기를 28일로해서 예측해본다. (28*3 = 84)\n",
    "    prediction_i = arima_main(test_df_daily, sampling_period_days=28, fcst_period=3)\n",
    "    \n",
    "    #모델 개발시 21일도 해본것으로 추측할 수 있다.\n",
    "    # if prediction_i is None:\n",
    "    #     prediction_i = arima_main(test_df_daily, sampling_period_days=21, fcst_period=4)\n",
    "    \n",
    "    #28일로 다운샘플링을 했을때 행이 충분하지 않다면 14일로 다운샘플링 하여 예측해본다. (14 *7 = 98)\n",
    "    if prediction_i is None:\n",
    "        prediction_i = arima_main(test_df_daily, sampling_period_days=14, fcst_period=7)\n",
    "    \n",
    "    #위와 비슷(7*12 = 84)\n",
    "    if prediction_i is None:\n",
    "        prediction_i = arima_main(test_df_daily, sampling_period_days=7, fcst_period=12)\n",
    "    \n",
    "    #다운샘플링 모두 실패시 ARIMA를 적용하지 않고 log 평균을 통해 직접 예측치를 계산한다.\n",
    "    if prediction_i is None:\n",
    "        \n",
    "        #14일로 구분짓고, 나누어떨어질 수 있도록 나머지에 해당하는 행은 버린다.\n",
    "        test_df = test_df_daily[len(test_df_daily) % 14:].resample('14D').sum()\n",
    "        \n",
    "        #no_sales 비율을 계산하여 이후 계산할 예측치에 대한 보정값으로 쓴다\n",
    "        prob_of_no_sales = len(test_df[(test_df.amount == 0) | (test_df.amount.isna())]) / len(test_df)\n",
    "        \n",
    "        #로그를 취한다.\n",
    "        ts_log = ts_log[~ts_log.isin([np.nan, np.inf, -np.inf])]\n",
    "        ts_log_wkly = np.log(test_df.amount)\n",
    "        \n",
    "        #예측값 계산\n",
    "        estimated_amt = np.exp(ts_log_wkly.mean() - ts_log_wkly.std() * optimal_z_score) * (1 - prob_of_no_sales)\n",
    "        \n",
    "        \n",
    "        prediction_i = estimated_amt * mean_period\n",
    "    #예측 결과를 제출파일에 저장한다.\n",
    "    submission_copy.loc[submission_copy['store_id'] == store_i, 'total_sales'] = prediction_i\n",
    "print(submission_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     store_id   total_sales\n",
      "0           0  68426.936910\n",
      "1           1  11840.490783\n",
      "2           2  16208.571184\n",
      "3           3  26870.446220\n",
      "4           4  11752.421019\n",
      "..        ...           ...\n",
      "195       195  31237.923574\n",
      "196       196  31340.097798\n",
      "197       197     57.921562\n",
      "198       198   1061.787577\n",
      "199       199  18906.142306\n",
      "\n",
      "[200 rows x 2 columns]\n",
      "submissionCopy1st.csv\n"
     ]
    }
   ],
   "source": [
    "#제출파일 저장\n",
    "#프로그램 전역변수값에 따라 자동으로 파일명을 변경.\n",
    "# output_file_name_fmt = '../1st_data/py_4arima_pos_sep_{optimal_p}-{sampling_period}_no_sales_prob&no mean{mean_period}&min_period {min_period}_pdq {max_pdq}.csv'\n",
    "# output_file_name = output_file_name_fmt.format(optimal_p=round(optimal_prob, 4),\n",
    "#                                                sampling_period=sampling_p,\n",
    "#                                                mean_period=mean_period,\n",
    "#                                                min_period=min_period,\n",
    "#                                                max_pdq=max_pdq)\n",
    "print(submission_copy)\n",
    "output_file_name = 'submissionCopy1st.csv';\n",
    "submission_copy.to_csv(output_file_name, index=False)\n",
    "print(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampling - 여러 행을 집계하여 예측의 단위를 줄임. \n",
    "#너무 묶으면 seanality가 사라짐. 예측을 해야되는 기간이 1년이 아니라 100일 이므로 \n",
    "#월별 Seasonality를 살려주는건 좋음. 요일별 Seasonality는 예측기간(100일)을 고려하면 그 영향도가 작음.\n",
    "#따라서 다운샘플링 기간을 최소 7일 최대 28일로 진행하는 것이 타당하다고 판단됨. \n",
    "#다운샘플링 기준 기간을 7일 14일 28일로 잡은 것은 시계열 데이터의 특성을 최대한 살릴 수 있었다고 생각함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
