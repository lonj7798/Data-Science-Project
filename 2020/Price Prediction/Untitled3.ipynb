{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.stats as st\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings(\"ignore\")  # specify to ignore warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../../Data/Price prediction/test.csv\")\n",
    "submission = pd.read_csv(\"../../Data/Price prediction/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.27\n"
     ]
    }
   ],
   "source": [
    "df_copy = test.copy()\n",
    "df_copy.date = pd.to_datetime(df_copy.date)\n",
    "\n",
    "# df_copy.date = pd.to_datetime(df_copy.date + \" \" + df_copy.time, format='%d/%m/%y %H:%M:%S')\n",
    "\n",
    "df_copy.date = pd.to_datetime(df_copy.date.astype(str) + \" \" + df_copy.time, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Remove negative values from the data set.\n",
    "\n",
    "def reduce_noise_by_removing_neg_vals(df_copy):\n",
    "    df_pos = df_copy[df_copy.amount > 0]\n",
    "    df_neg = df_copy[df_copy.amount < 0]\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    for nega_i in df_neg.to_records()[:]:\n",
    "        store_i = nega_i[1]\n",
    "        date_i = nega_i[2]\n",
    "        card_i = nega_i[4]\n",
    "        amt_i = nega_i[5]\n",
    "        row_i = df_pos[df_pos.store_id == store_i]\n",
    "        row_i = row_i[row_i.card_id == card_i]\n",
    "        row_i = row_i[row_i.amount >= abs(amt_i)]\n",
    "        row_i = row_i[row_i.date <= date_i]\n",
    "        if len(row_i[row_i.amount == abs(amt_i)]) > 0:\n",
    "            row_i = row_i[row_i.amount == abs(amt_i)]\n",
    "            matched_row = row_i[row_i.date == max(row_i.date)]\n",
    "            # df_pos.loc[matched_row.index, 'amount'] = 0\n",
    "            df_pos = df_pos.loc[~df_pos.index.isin(matched_row.index), :]\n",
    "        elif len(row_i[row_i.amount > abs(amt_i)]) > 0:\n",
    "            matched_row = row_i[row_i.date == max(row_i.date)]\n",
    "            df_pos.loc[matched_row.index, 'amount'] = matched_row.amount + amt_i\n",
    "        # else:\n",
    "        #     pass\n",
    "            # no_match.append(nega_i)\n",
    "    end = datetime.now()\n",
    "    time_took = (end - start).seconds / 60\n",
    "\n",
    "    print(round(time_took, 2))\n",
    "    return df_pos\n",
    "\n",
    "df_pos = reduce_noise_by_removing_neg_vals(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(y):\n",
    "    # perform Augmented Dickey Fuller test\n",
    "    print('Results of Augmented Dickey-Fuller test:')\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value ({})'.format(key)] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "def ts_diagnostics(y, lags=None, title='', filename=''):\n",
    "    '''\n",
    "    Calculate acf, pacf, qq plot and Augmented Dickey Fuller test for a given time series\n",
    "    '''\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    # weekly moving averages (5 day window because of workdays)\n",
    "    rolling_mean = pd.Series.rolling(y, window=2).mean()\n",
    "    rolling_std = pd.Series.rolling(y, window=2).std()\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 12))\n",
    "    layout = (3, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "    qq_ax = plt.subplot2grid(layout, (2, 0))\n",
    "    hist_ax = plt.subplot2grid(layout, (2, 1))\n",
    "\n",
    "    # time series plot\n",
    "    y.plot(ax=ts_ax)\n",
    "    rolling_mean.plot(ax=ts_ax, color='crimson')\n",
    "    rolling_std.plot(ax=ts_ax, color='darkslateblue')\n",
    "    plt.legend(loc='best')\n",
    "    ts_ax.set_title(title, fontsize=24)\n",
    "\n",
    "    # acf and pacf\n",
    "    plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n",
    "    plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n",
    "\n",
    "    # qq plot\n",
    "    sm.qqplot(y, line='s', ax=qq_ax)\n",
    "    qq_ax.set_title('QQ Plot')\n",
    "\n",
    "    # hist plot\n",
    "    y.plot(ax=hist_ax, kind='hist', bins=25)\n",
    "    hist_ax.set_title('Histogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # perform Augmented Dickey Fuller test\n",
    "    print('Results of Dickey-Fuller test:')\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)\n",
    "    return\n",
    "\n",
    "df = test.copy()\n",
    "test_groupby_date_store = df.groupby(['date', 'store_id'])[['amount', 'holyday']].sum()\n",
    "test_groupby_date_store = test_groupby_date_store.reset_index()\n",
    "\n",
    "test_groupby_date_store = test_groupby_date_store.set_index('date')\n",
    "store_list = test_groupby_date_store.store_id.unique()\n",
    "\n",
    "store_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_groupby_date_store.index = pd.to_datetime(test_groupby_date_store.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9580957780787686\n",
      "1.7290036387221377\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_params(y):\n",
    "    # Define the p, d and q parameters to take any value between 0 and 1\n",
    "\n",
    "    param_dict = {}\n",
    "    for param in pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            )\n",
    "            results = mod.fit()\n",
    "            model = ARIMA(y, order=param)\n",
    "            results_ARIMA = model.fit(disp=-1)\n",
    "            results_ARIMA.summary()\n",
    "            param_dict[results.aic] = param\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    min_aic = min(param_dict.keys())\n",
    "    optimal_params = param_dict[min_aic]\n",
    "    return optimal_params\n",
    "\n",
    "\n",
    "sampling_p = 28\n",
    "mean_period = 2 * 3 #14 * 2*3\n",
    "\n",
    "predic_len = math.floor(100 / sampling_p)\n",
    "\n",
    "expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "expected_loss_pct_lending = 1.00\n",
    "optimal_prob = expected_loss_pct_lending / (expected_loss_pct_lending + expected_return_pct_lending)\n",
    "optimal_z_score = st.norm.ppf(optimal_prob)\n",
    "\n",
    "min_period = 6\n",
    "\n",
    "\n",
    "max_pdq = 2\n",
    "p = d = q = range(0, max_pdq)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "\n",
    "pdqs = dict()\n",
    "print(optimal_prob)\n",
    "print(optimal_z_score)\n",
    "output_file_name_fmt = '../1st_data/py_4arima_pos_sep_{optimal_p}-{sampling_period}_no_sales_prob&no mean{mean_period}&min_period {min_period}_pdq{max_pdq}.csv'\n",
    "output_file_name = output_file_name_fmt.format(optimal_p=round(optimal_prob, 4),\n",
    "                                               sampling_period=sampling_p,\n",
    "                                               mean_period=mean_period,\n",
    "                                               min_period=min_period,\n",
    "                                               max_pdq=max_pdq)\n",
    "\n",
    "submission_copy = submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../1st_data/py_4arima_pos_sep_0.9581-28_no_sales_prob&no mean6&min_period 6_pdq2.csv\n"
     ]
    }
   ],
   "source": [
    "def arima_main(input_df, sampling_period_days, fcst_period):\n",
    "    input_df = input_df[len(input_df) % sampling_period_days:].resample(str(sampling_period_days) + 'D').sum()\n",
    "    prob_of_no_sales = len(input_df[(input_df.amount == 0) | (input_df.amount.isna())]) / len(input_df)\n",
    "    ts_log = np.log(input_df.amount)\n",
    "    ts_log = ts_log[~ts_log.isin([np.nan, np.inf, -np.inf])]\n",
    "\n",
    "    if len(ts_log) < min_period:\n",
    "        return None\n",
    "    if sampling_period_days >= 28:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "    elif sampling_period_days >= 14:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 14) / 365\n",
    "    else:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "\n",
    "    expected_loss_pct_lending = 1.00\n",
    "    optimal_prob = expected_loss_pct_lending / (expected_loss_pct_lending + expected_return_pct_lending)\n",
    "    optimal_z_score = st.norm.ppf(optimal_prob)\n",
    "\n",
    "    optimal_params = get_optimal_params(ts_log)\n",
    "    pdqs[store_i] = optimal_params\n",
    "\n",
    "    model = ARIMA(ts_log, order=optimal_params)\n",
    "    results_ARIMA = model.fit(disp=-1)\n",
    "    fcst = results_ARIMA.forecast(fcst_period)\n",
    "\n",
    "    fcst_means = fcst[0]\n",
    "    fcst_stds = fcst[1]\n",
    "    fcst_i = fcst_means - (fcst_stds * optimal_z_score)\n",
    "    fcst_i = sum(map(lambda x: np.exp(x) if np.exp(x) > 0 else 0, fcst_i))\n",
    "    prediction_i = fcst_i * (1 - prob_of_no_sales)\n",
    "    return prediction_i\n",
    "\n",
    "for store_i in store_list[:]:\n",
    "    prediction_i = None\n",
    "    test_df = test_groupby_date_store[test_groupby_date_store.store_id == store_i]\n",
    "    test_df_daily = test_df.resample('D').sum()\n",
    "    prediction_i = arima_main(test_df_daily, sampling_period_days=28, fcst_period=3)\n",
    "    # if prediction_i is None:\n",
    "    #     prediction_i = arima_main(test_df_daily, sampling_period_days=21, fcst_period=4)\n",
    "    if prediction_i is None:\n",
    "        prediction_i = arima_main(test_df_daily, sampling_period_days=14, fcst_period=7)\n",
    "    if prediction_i is None:\n",
    "        prediction_i = arima_main(test_df_daily, sampling_period_days=7, fcst_period=12)\n",
    "    if prediction_i is None:\n",
    "        test_df = test_df_daily[len(test_df_daily) % 14:].resample('14D').sum()\n",
    "\n",
    "        prob_of_no_sales = len(test_df[(test_df.amount == 0) | (test_df.amount.isna())]) / len(test_df)\n",
    "        ts_log = ts_log[~ts_log.isin([np.nan, np.inf, -np.inf])]\n",
    "        ts_log_wkly = np.log(test_df.amount)\n",
    "\n",
    "        estimated_amt = np.exp(ts_log_wkly.mean() - ts_log_wkly.std() * optimal_z_score) * (1 - prob_of_no_sales)\n",
    "        prediction_i = estimated_amt * mean_period\n",
    "\n",
    "    submission_copy.loc[submission_copy['store_id'] == store_i, 'total_sales'] = prediction_i\n",
    "\n",
    "submission_copy.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
